{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple deployment and inference\n",
    "In this notebook we will walk through a simple deployment to inference on a model. For this example we will be using an open source model that uses an Aloha CNN LSTM model for classifiying Domain names.\n",
    "In this notebook we will take the following steps\n",
    "<ol>\n",
    "    <li>Open connection to wallaroo</li>\n",
    "    <li>upload a model</li>\n",
    "    <li>create a deployment for inferencing on a model</li>\n",
    "    <li>run inferences</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open connection to wallaroo\n",
    "here we will import the libraries needed for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WALLAROO_SDK_CREDENTIALS\"] = 'creds.json'\n",
    "wl = wallaroo.Client(auth_type=\"user_password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start client to cluster\n",
    "We can now us the wallaroo library to set up a connection to the wallaroo cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table>\n",
       "            <tr>\n",
       "                <th>Name</th>\n",
       "                <th>Created At</th>\n",
       "                <th>Users</th>\n",
       "                <th>Models</th>\n",
       "                <th>Pipelines</th>\n",
       "            </tr>\n",
       "            \n",
       "        <tr >\n",
       "            <td>demandcurve-workspace</td>\n",
       "            <td>2022-03-28 16:22:42</td>\n",
       "            <td>['steve@ex.co']</td>\n",
       "            <td>3</td>\n",
       "            <td>1</td>\n",
       "        </tr>\n",
       "        \n",
       "\n",
       "        <tr >\n",
       "            <td>demandcurve-workspace</td>\n",
       "            <td>2022-03-28 16:28:21</td>\n",
       "            <td>['steve@ex.co']</td>\n",
       "            <td>3</td>\n",
       "            <td>1</td>\n",
       "        </tr>\n",
       "        \n",
       "\n",
       "        <tr >\n",
       "            <td>imdb-workspace</td>\n",
       "            <td>2022-03-28 17:44:32</td>\n",
       "            <td>['steve@ex.co']</td>\n",
       "            <td>0</td>\n",
       "            <td>0</td>\n",
       "        </tr>\n",
       "        \n",
       "\n",
       "        <tr >\n",
       "            <td>imdb-workspace</td>\n",
       "            <td>2022-03-28 18:59:43</td>\n",
       "            <td>['steve@ex.co']</td>\n",
       "            <td>2</td>\n",
       "            <td>1</td>\n",
       "        </tr>\n",
       "        \n",
       "\n",
       "        <tr >\n",
       "            <td>imdb-workspace</td>\n",
       "            <td>2022-03-28 19:23:04</td>\n",
       "            <td>['steve@ex.co']</td>\n",
       "            <td>2</td>\n",
       "            <td>1</td>\n",
       "        </tr>\n",
       "        \n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "[{'name': 'demandcurve-workspace', 'id': 1, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-28T16:22:42.312533+00:00', 'models': [{'name': 'demandcurve', 'version': 'a7695994-36f0-4142-8da4-b818c70ba5b4', 'file_name': 'demand_curve_v1.onnx', 'last_update_time': datetime.datetime(2022, 3, 28, 16, 23, 29, 344038, tzinfo=tzutc())}, {'name': 'preprocess', 'version': '65a888f8-e44f-4d40-8ee8-4f5bc953ce79', 'file_name': 'preprocess.py', 'last_update_time': datetime.datetime(2022, 3, 28, 16, 23, 30, 124241, tzinfo=tzutc())}, {'name': 'postprocess', 'version': 'ec94775a-3372-485c-b69a-3dc0bf51a381', 'file_name': 'postprocess.py', 'last_update_time': datetime.datetime(2022, 3, 28, 16, 23, 30, 958720, tzinfo=tzutc())}], 'pipelines': [{'name': 'demand-curve-pipeline', 'create_time': datetime.datetime(2022, 3, 28, 16, 23, 35, 254636, tzinfo=tzutc()), 'definition': '[]'}]},\n",
       " {'name': 'demandcurve-workspace', 'id': 2, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-28T16:28:21.625896+00:00', 'models': [{'name': 'demandcurve', 'version': '9cd1fcae-1fa1-4e12-8e67-d4a67f240a46', 'file_name': 'demand_curve_v1.onnx', 'last_update_time': datetime.datetime(2022, 3, 28, 16, 28, 21, 723933, tzinfo=tzutc())}, {'name': 'preprocess', 'version': 'b1f51290-ac47-4289-8a55-310507d52af5', 'file_name': 'preprocess.py', 'last_update_time': datetime.datetime(2022, 3, 28, 16, 28, 22, 19745, tzinfo=tzutc())}, {'name': 'postprocess', 'version': '06e79dfe-623e-482e-95f6-bd6fa1b26264', 'file_name': 'postprocess.py', 'last_update_time': datetime.datetime(2022, 3, 28, 16, 28, 22, 167192, tzinfo=tzutc())}], 'pipelines': [{'name': 'demand-curve-pipeline', 'create_time': datetime.datetime(2022, 3, 28, 16, 28, 22, 313553, tzinfo=tzutc()), 'definition': '[]'}]},\n",
       " {'name': 'imdb-workspace', 'id': 3, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-28T17:44:32.109083+00:00', 'models': [], 'pipelines': []},\n",
       " {'name': 'imdb-workspace', 'id': 4, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-28T18:59:43.388522+00:00', 'models': [{'name': 'embedder-o', 'version': '80aaec71-7bb4-49c6-84b5-9fe132e80ca0', 'file_name': 'embedder.onnx', 'last_update_time': datetime.datetime(2022, 3, 28, 18, 59, 48, 489813, tzinfo=tzutc())}, {'name': 'smodel-o', 'version': '1b46b1ba-5e4e-4e54-b247-7f54fde15685', 'file_name': 'sentiment_model.onnx', 'last_update_time': datetime.datetime(2022, 3, 28, 18, 59, 48, 734772, tzinfo=tzutc())}], 'pipelines': [{'name': 'imdb-pipeline', 'create_time': datetime.datetime(2022, 3, 28, 19, 0, 13, 909842, tzinfo=tzutc()), 'definition': '[]'}]},\n",
       " {'name': 'imdb-workspace', 'id': 5, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-28T19:23:04.537027+00:00', 'models': [{'name': 'embedder-o', 'version': '4a7495d6-803e-439d-890d-7ea283480da4', 'file_name': 'embedder.onnx', 'last_update_time': datetime.datetime(2022, 3, 28, 19, 23, 4, 657393, tzinfo=tzutc())}, {'name': 'smodel-o', 'version': '7efac097-fd1f-44bf-8004-76711e667413', 'file_name': 'sentiment_model.onnx', 'last_update_time': datetime.datetime(2022, 3, 28, 19, 23, 4, 901949, tzinfo=tzutc())}], 'pipelines': [{'name': 'imdb-pipeline', 'create_time': datetime.datetime(2022, 3, 28, 19, 23, 5, 71670, tzinfo=tzutc()), 'definition': '[]'}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.list_workspaces()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work space demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_workspace = wl.create_workspace(\"aloha-workspace\")\n",
    "_ = wl.set_current_workspace(new_workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the workspace is created and running with the `get_current_workspace()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'aloha-workspace', 'id': 6, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-29T16:14:08.85824+00:00', 'models': [], 'pipelines': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.get_current_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config \n",
    "before deploying an inference engine we will set the configuration of the engine.\n",
    "To do this we will use the wallaroo DeploymentConfigBuilder() and fill in the options listed below to determine what the properties of our inference engine will be\n",
    "\n",
    "note: this will not start the process of building anything in the kubernetes cluster yet. we are just setting the deployment configuration we will want to use later.\n",
    "- replica_count - 1 => when deployed this will have a single inference engine\n",
    "- cpus - 4 => each inference engine will have 4 cpus\n",
    "- memory - 8Gi => each inference engine will have 8 Gb of memory\n",
    "\n",
    "# recommedations\n",
    "for this we are going to create two deployment_configrations \n",
    " - deployment_config\n",
    " -- this config will use a single replica with 4 cpus and 8 Gb of memory and will be used for our deployment later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(4).memory(\"8Gi\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# upload models\n",
    "to get started we will upload 4 models(for simplicity it will be the 4 models with different names assigned to it.\n",
    "once the models are uploaded we can select any model we wish and create a deployment with it.\n",
    "\n",
    "Note that for this example we are applying the model from a .ZIP file.  This is a [protobuf](https://developers.google.com/protocol-buffers) file that has been defined for evaluating web pages through tensorflow data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wl.upload_model(\"aloha-2\", \"./aloha-cnn-lstm.zip\").configure(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# viewing and selecting a model\n",
    "now that we have uploaded several models we can produce a list of them with the wallaroo library by using the list_models() tool. we can then view the models names with model_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: aloha-2\n",
      "2022-03-29 16:14:22.076073+00:00\n",
      "----------------------\n",
      "model name: smodel-o\n",
      "2022-03-28 19:23:04.901949+00:00\n",
      "----------------------\n",
      "model name: embedder-o\n",
      "2022-03-28 19:23:04.657393+00:00\n",
      "----------------------\n",
      "model name: smodel-o\n",
      "2022-03-28 18:59:48.734772+00:00\n",
      "----------------------\n",
      "model name: embedder-o\n",
      "2022-03-28 18:59:48.489813+00:00\n",
      "----------------------\n",
      "model name: postprocess\n",
      "2022-03-28 16:28:22.167192+00:00\n",
      "----------------------\n",
      "model name: preprocess\n",
      "2022-03-28 16:28:22.019745+00:00\n",
      "----------------------\n",
      "model name: demandcurve\n",
      "2022-03-28 16:28:21.723933+00:00\n",
      "----------------------\n",
      "model name: postprocess\n",
      "2022-03-28 16:23:30.958720+00:00\n",
      "----------------------\n",
      "model name: preprocess\n",
      "2022-03-28 16:23:30.124241+00:00\n",
      "----------------------\n",
      "model name: demandcurve\n",
      "2022-03-28 16:23:29.344038+00:00\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "for m in wl.list_models():\n",
    "    print(\"model name: \" + m.name())\n",
    "    print(str(m.last_update_time()))\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a model\n",
    "Now that we have a model that we want to use we will create a deployment for it. \n",
    "\n",
    "We will tell the deployment we are using a tensorflow model and give the deployment name and the configuration we want for the deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Now that our models are uploaded, we'll create our pipeline that can ingest the data, pass the data through each of the pipelines steps and give us a final output.  In this case, we only have one step of applying the inputted data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 45s ....... ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'aloha-test-demo', 'create_time': datetime.datetime(2022, 3, 29, 16, 15, 31, 638290, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'aloha-2', 'version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde', 'sha': 'fd998cd5e4964bbbb4f8d29d245a8ac67df81b62be767afbceb96a03d1a01520'}]}}]\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline = wl.build_pipeline('aloha-test-demo')\n",
    "aloha_pipeline.add_model_step(model)\n",
    "aloha_pipeline.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the pipeline is running and list what models are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': None,\n",
       " 'engines': [{'ip': '10.12.1.236',\n",
       "   'name': 'engine-864d86d898-k26hv',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'aloha-test-demo',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'aloha-2',\n",
       "      'version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde',\n",
       "      'sha': 'fd998cd5e4964bbbb4f8d29d245a8ac67df81b62be767afbceb96a03d1a01520',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.12.1.235',\n",
       "   'name': 'engine-lb-85846c64f8-dcj4f',\n",
       "   'status': 'Running',\n",
       "   'reason': None}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Successful deployment\n",
    "now that we have a deployment running we start inferencing.\n",
    "\n",
    "* **Note**:  If you receive an error about running out of resources, undeploy any other pipelines.  This command can quickly undeploy all pipelines to regain resources - through it should not be run in a production environment for that reason:\n",
    "\n",
    "```python\n",
    "for p in wl.list_pipelines(): p.undeploy()\n",
    "```\n",
    "\n",
    "## infer 1 row\n",
    "\n",
    "Now that the pipeline is deployed and our Aloha models are in place, we'll perform a test to verify that everythig is running.  We'll use the `infer_from_file` command to load a single encoded URL into the inference engine and print the results back out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InferenceResult({'check_failures': [],\n",
       "  'elapsed': 631348351,\n",
       "  'model_name': 'aloha-2',\n",
       "  'model_version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde',\n",
       "  'original_data': {'text_input': [[0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    28,\n",
       "                                    16,\n",
       "                                    32,\n",
       "                                    23,\n",
       "                                    29,\n",
       "                                    32,\n",
       "                                    30,\n",
       "                                    19,\n",
       "                                    26,\n",
       "                                    17]]},\n",
       "  'outputs': [{'Float': {'data': [0.001519620418548584], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.9829147458076477], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.012099534273147583], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [4.7593468480044976e-05],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [2.0289742678869516e-05],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [0.0003197789192199707],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [0.011029303073883057], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.9975639581680298], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.010341644287109375], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.008038878440856934], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.016155093908309937], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.006236225366592407], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.0009985864162445068],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [1.7933435344117743e-26],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [1.388984431455466e-27],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}}],\n",
       "  'pipeline_name': 'aloha-test-demo',\n",
       "  'time': 1648570552486})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.infer_from_file(\"data-1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run larger batch\n",
    "\n",
    "Now that our smoke test is succesful, let's really give it some data.  We have two inference files we can use:\n",
    "\n",
    "* `data-1k.json`:  Contains 1,0000 inferences\n",
    "* `data-25k.json`: Contains 25,000 inferences\n",
    "\n",
    "We'll pipe both of these files through the `aloha_pipeline` deployment URL, and place the results in a file named `response.txt`.  We'll also display the time this takes.  Note that for larger batches of 50,000 inferences or more can be difficult to view in Juypter Hub because of its size.\n",
    "\n",
    "When running this example, replace the URL from the `_deployment._url()` command into the `curl` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://engine-lb.aloha-test-demo-5:29502/pipelines/aloha-test-demo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline._deployment._url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.9M  100 10.1M  100 2886k   539k   149k  0:00:19  0:00:19 --:--:-- 2570k\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://engine-lb.aloha-test-demo-5:29502/pipelines/aloha-test-demo -H \"Content-Type:application/json\" --data @data-25k.json > curl_response.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## undeploy model\n",
    "this will take down our inference engine. and free up the resources in kubernetes\n",
    "- note that if the deployment variable is unchanged deployment.deploy() will restart the inference engine in the same configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'aloha-test-demo', 'create_time': datetime.datetime(2022, 3, 29, 16, 15, 31, 638290, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'aloha-2', 'version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde', 'sha': 'fd998cd5e4964bbbb4f8d29d245a8ac67df81b62be767afbceb96a03d1a01520'}]}}]\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
