{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **A Simple Linear Regression Model Notebook for the Wallaroo Platform**\n",
    "#### A Comprehensive Tutorial for:\n",
    "1. Building a Linear Regression Model\n",
    "2. Deploying the Model into Wallaroo\n",
    "3. Using Wallaroo's Monitoring Capabilities to Analyze the Model.\n",
    "\n",
    "### The data was used from GC Wkshps\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Room+Occupancy+Estimation\n",
    "\n",
    "Adarsh Pal Singh, Vivek Jain, Sachin Chaudhari, Frank Alexander Kraemer, Stefan Werner and Vishal Garg, \"Machine Learning-Based Occupancy Estimation Using Multivariate Sensor Nodes,\" in 2018 IEEE Globecom Workshops (GC Wkshps), 2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building a Simple Linear Regression Model**\n",
    "\n",
    "This model will be built from a data set of sensor values and the occupancy of the room the sensors are in.\n",
    "The goal is to use the sensor data to predict the room occupancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing the Necessary Python Libraries**\n",
    "\n",
    "We will use a variety of libraries to implement the linear regression model\n",
    "\n",
    "#### These libraries include:\n",
    "- matplotlib\n",
    "- numpy\n",
    "- sklearn\n",
    "- pandas\n",
    "- onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Source: Us\n",
    "\n",
    "# Needed for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Needed for data tuning\n",
    "import numpy as np\n",
    "\n",
    "# Needed for creating the linear regression model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Needed for metrics of the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Needed for csv importing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing the Data Set using Pandas**\n",
    "The first step creating a linear regression model is read in the dataset using the pandas library  \n",
    "The `read_csv` method is responsible for reading in the data and `head()` method acesses the first few rows in the data  \n",
    "When picking a variable from the data we'll use `.corr()` to find which variable has the best correlation in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and displaying the dataset\n",
    "data = pd.read_csv('Occupancy_Estimation.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the correlations between each and every variables\n",
    "correlations = data.corr()\n",
    "correlations[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pulling our Independent (x) and Dependent (y) Variables from the DataFrame**\n",
    "Next we are going to access the Independent variable `S1_Light` and Dependent variable `Room_Occupancy_Count` to be stored in the x and y value  \n",
    "The `values` function accesses the values in the dataset at the index of the given variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of features x and prints data x\n",
    "x = data[['S1_Light']].values\n",
    "print('S1_Light Values:')\n",
    "print(x)\n",
    "\n",
    "# Array of independent variable y and prints data y\n",
    "y = data['Room_Occupancy_Count'].values\n",
    "print('\\nRoom_Occupancy_Count Values:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the Data Set into Testing and Training Subsets**\n",
    "Next we use the `train_test_split()` method in the sklearn library to split the data into test and train sets \n",
    "The train_test_split gives test/train data to x and y  \n",
    "Create test and train datasets with 0.2 (20%) of the dataset being test data  \n",
    "The `random_state` decides which indices of data to pull from  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The train_test_split gives test/train data to x and y\n",
    "# Create test and train datasets with 0.2 (20%) of the dataset being test data\n",
    "# The random_state decides which indices of data to pull from\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Linear Regression Model using Sklearn and Fitting our Training Data to the Model**\n",
    "Now using `LinearRegression()` method we create a linear regression object which we call `regr`  \n",
    "Then we take `.fit(x_train, y_train)` method uses x and y train data as parameters to see how well it fits the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creating the linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# The regr.fit() measures how well the x and y train data fit the model\n",
    "regr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predicting the Room Occupancy from our Independent Variable Test Set**\n",
    "In this step we take in the independent variable test set for a the parameter in the `predict()` method in order to predict the outcome for the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regr.predict() creates a prediction based on the x test data\n",
    "y_pred = regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finding the Metrics to Analyze the Prediction**\n",
    "In this step we use various functions and methods in order to see how well our linear regression model is predicting our data. The  `coef_` function tells us the **correlation coefficient**, which shows in what way our variables correlate with each other. Next up we have the `mean_squared_error()` method, which shows us the distance from the estimated values and the true values; The best possible score would be 0. Lastly there's the `r2_score()` method which is responsible for displaying how well our data fits the current model, with an R^2 score of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the coefficients\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n",
    "\n",
    "# Prints the mean squared error\n",
    "print(\"Root mean squared error: %.2f\" % mean_squared_error(y_test, y_pred, squared=False))\n",
    "\n",
    "# Prints the coefficient of determination: 1 is perfect prediction\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting the Linear Regression**\n",
    "Using `plt` in **matplotlib** library we can print out the different aspects of our linear regression graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plt.scatter() plots the x and y test points in the linear regression model\n",
    "plt.scatter(x_test, y_test, color=\"black\")\n",
    "\n",
    "# The plt.plot() creates the line of best fit\n",
    "plt.plot(x_test, y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "# Sets the x-axis, y-axis, and title of the model\n",
    "plt.xlabel('S1_Light')\n",
    "plt.ylabel('Room Occupancy')\n",
    "plt.title('Room Occupancy VS S1_Light Sensor')\n",
    "\n",
    "# The plt.show() displays the model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deploying the Model into Wallaroo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting the Sklearn Model into Onnx for use on the Wallaroo Platform**\n",
    "For the next step refer to [sklearn-regression-to-onnx tutorial](https://docs.wallaroo.ai/wallaroo-tutorials/conversion-tutorials/sklearn-regression-to-onnx/) in the wallaroo documentation for how to convert file to onnx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for the conversion process\n",
    "import onnx, skl2onnx, onnxmltools\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx.common.data_types import DoubleTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model_to_onnx converts the model to onnx to be upload to Wallaroo Platfrom\n",
    "# For more detailed steps refer to \"model_conversion\"\n",
    "def model_to_onnx(model, cols, *, input_type='Double'):\n",
    "    input_type_lower=input_type.lower()\n",
    "    # How to manage float values\n",
    "    if input_type=='Double':\n",
    "        tensor_type=DoubleTensorType\n",
    "    elif input_type=='Float':\n",
    "        tensor_type=FloatTensorType\n",
    "    else:\n",
    "        raise ValueError(\"bad input type\")\n",
    "    tensor_size=cols\n",
    "    initial_type=[(f'{input_type_lower}_input', tensor_type([None, tensor_size]))]\n",
    "    onnx_model=onnxmltools.convert_sklearn(model,initial_types=initial_type)\n",
    "    return onnx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model_to_onnx() takes the pickle file and converts it to onnx\n",
    "onnx_model_converted = model_to_onnx(regr, 1)\n",
    "\n",
    "# The onnx.save_model() saves the converted model into a file\n",
    "onnx.save_model(onnx_model_converted, \"occupancy_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing into Wallaroo\n",
    "Reference the [Wallaroo 101 Tutorial](https://docs.wallaroo.ai/wallaroo-101/) for how to access the wallaroo platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for the use of Wallaroo\n",
    "import wallaroo\n",
    "\n",
    "# The wallaroo.Client() allows the file to access wallaroo platform\n",
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the name for workspace, pipeline, and model\n",
    "workspace_name = 'msuproject3'\n",
    "pipeline_name = 'occupancypipeline2'\n",
    "model_name = 'occupancymodel2'\n",
    "\n",
    "# Created to fetch the model\n",
    "model_file_name = 'occupancy_model.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_workspace() gets/create the workspace when needed\n",
    "# For more detailed steps refer to \"wallaroo-101\"\n",
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "# The get_pipeline() gets/create the pipeline when needed\n",
    "# For more detailed steps refer to \"wallaroo-101\"\n",
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(pipeline_name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(pipeline_name)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls function to create workspace\n",
    "workspace = get_workspace(workspace_name)\n",
    "\n",
    "# The wl.set_current_workspace() sets the workspace to currently being worked on\n",
    "set_workspace = wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thw wl.list_workspaces() prints the lists of the workspaces\n",
    "wl.list_workspaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.set_current_workspace(workspace)\n",
    "#gw = wl.get_current_workspace()\n",
    "# wl.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Pipeline and Uploading the Model\n",
    "In this step we are using `build_pipeline()`. Here we create the pipeline by giving the method a string. We defined `pipeline_name` earlier.\n",
    "\n",
    "To upload the model we use `upload_model()`. Here we need give a string, and a file. Both we defined earlier in the tutorial.\n",
    "\n",
    "Lastly, we add the model as a step to the pipeline using `add_model_step()`. All we have to give here is give the function our model we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wl.get_pipeline() creates the pipeline\n",
    "occupancy_pipeline = wl.build_pipeline(pipeline_name)\n",
    "\n",
    "# The wl.upload_model() uploads the model to the platform\n",
    "occupancy_model = wl.upload_model(model_name, model_file_name).configure()\n",
    "\n",
    "# The occuupancy_pipeline.add_model_step() adds the model to pipeline to be deployed\n",
    "occupancy_pipeline = occupancy_pipeline.add_model_step(occupancy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Validation to the model\n",
    "The `add_validation()` takes in two parameters.  \n",
    "\n",
    "**add_validation**(**string** name, **bool** condition)  \n",
    "\n",
    "**Parameters:**\n",
    "* **Name**: This must be lower-case. It is an arbitrary name used to name the condition.  \n",
    "* **Condition**: This describes the condition you want to **NOT** mark as an anomaly. First, give the output values and if greater/less/equal to a chosen value.  \n",
    "\n",
    "The validation step must be implemented before deploying the pipeline and before adding a post process step (In the current version of Wallaroo, this is a work-around to a known issue).  \n",
    "\n",
    "In our code, we name our validation as `no_negative_people`. The condition takes in our `occupancy_model` outputs and compares it to the `float(0)`. This needs to be a float because `occupancy_model` will output floats. So we are only allowing the values greater than or equal to 0.0 to not be marked as an anomaly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_pipeline = occupancy_pipeline.add_validation(\n",
    "                                        'no_negative_people',\n",
    "                                        occupancy_model.outputs[0][0] >= float(0)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Post-Process\n",
    "The post process step formats the data. We are able to implemennt whatever formatting rules we want.\n",
    "\n",
    "In this tutorial, since we cannot have a fraction of a person, we implement rounding.\n",
    "\n",
    "Here we are uploading another \"model\" to our pipeline.\n",
    "\n",
    "First, we must take our `postproccess.py` file and upload it to Wallaroo using `wl.upload_model()`. We are naming this model `postprocess`. We are also sending our `postprocess.py` file. This time we must also configure it as a python file. \n",
    "\n",
    "We then need to add the model to our pipeline using `add_model_step()` as we did before with our model.\n",
    "Except this time, we upload `module_post`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_post = wl.upload_model(\"postprocess\", \"./postprocess.py\").configure('python')\n",
    "\n",
    "occupancy_pipeline = occupancy_pipeline.add_model_step(module_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the Model\n",
    "We are close to the final step now. Here we utilize Wallaroo's amazing cluster to run our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The occupancy_pipeline.deploy() activating the pipeline\n",
    "occupancy_pipeline.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the deployment\n",
    "This can be a usefull line of code, to ensure that the pipeline is running succesfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The occupancy_pipeline.status() displays the status of the pipeline\n",
    "occupancy_pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an Inference\n",
    "\n",
    "Finally, we get to utilize all of our work and put it into production.\n",
    "\n",
    "In this first box, we are just implementing a function that will format our data into a dictionary format. This format is needed in order to properly run inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for the infrences\n",
    "import json\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "# The pandas_to_dict() converts the values into dictionary for infrences\n",
    "def pandas_to_dict(df):\n",
    "    input_dict = {\n",
    "    'tensor': df.to_numpy().tolist()\n",
    "    }\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data\n",
    "\n",
    "Here we just need to grab the data from our .csv that we want to infer upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Single Datums\n",
    "\n",
    "In our setting, we will pretend that our sensors are sending our pipeline a single datum at a time. In our scenrio, we will send a good datum (something that won't trigger a validation error) and we will also send a bad datum (something that will trigger a validation error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data = pandas_to_dict(pd.read_csv('Good_Datum.csv').iloc[:,6:7])\n",
    "bad_data = pandas_to_dict(pd.read_csv('Bad_Datum.csv').iloc[:,6:7])\n",
    "#sneak_data = pandas_to_dict(pd.read_csv('All_Data_with_sneaky_sneak.csv').iloc[:,6:7])\n",
    "\n",
    "print(good_data)\n",
    "print(sneak_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the inference\n",
    "\n",
    "`occupancy_pipeline.infer()`  \n",
    "This method takes our input data that we pulled from our data, and creates the inference. \n",
    "\n",
    "The result shows the data we placed in, the outputs from the inference, and lastly, our post process step that rounds the numbers.\n",
    "\n",
    "`result[0].data()[0].tolist()`  \n",
    "This makes the result a little easier to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The occupancy_pipeline.infer() creates a result based on data given\n",
    "good_result = occupancy_pipeline.infer(good_data)\n",
    "good_result\n",
    "\n",
    "print(\"Finalized Data\")\n",
    "good_result[0].data()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The occupancy_pipeline.infer() creates a result based on data given\n",
    "bad_result = occupancy_pipeline.infer(bad_data)\n",
    "bad_result\n",
    "\n",
    "print(\"Finalized Data\")\n",
    "bad_result[0].data()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The occupancy_pipeline.infer() creates a result based on data given\n",
    "# sneak_bad_result = occupancy_pipeline.infer(sneak_data)\n",
    "# sneak_bad_result\n",
    "\n",
    "# print(\"Finalized Data\")\n",
    "# sneak_bad_result[0].data()[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undeploy the pipeline\n",
    "\n",
    "This part is crucial, you do not want take up more resources than you meant to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The occupancy_pipeline.undeploy() deactivates the pipeline\n",
    "occupancy_pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "\n",
    "`.logs()`\n",
    "\n",
    "Lastly, the pipeline logs can be deployed, even after the pipeline is undeployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = occupancy_pipeline.logs()\n",
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
